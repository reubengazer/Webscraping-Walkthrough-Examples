{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **15 Minute Webscraping in Python**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Author:** Reuben S. Gazer, AltaML  \n",
    "**Date Started:** June 6, 2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There have been many occasions where I need to \"scrape\" data from a website or set of website pages.  \n",
    "In this notebook, we will look at a few different ways to to web-scraping beginning with a simple retrieval and parsing of an HTML document.\n",
    "\n",
    "The entire process of scraping data from a web-page boils down to these steps:\n",
    "\n",
    "- **Step #1**: Identify the website URL from which you'd like to scrape data\n",
    "- **Step #2**: Ask the website to give you the raw HTML document\n",
    "- **Step #3**: Figure out which HTML-elements in the document contain the data you'd like to scrape\n",
    "- **Step #4**: Extract the data from these HTML-elements and store them in some convenient way"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Python \"requests\" Package: Basic HTML Retrieval (...but with problems!)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simplest way to retrieve the HTML content from a web-page is to use python's built-in _requests_ package.  \n",
    "For example, let's pull the HTML content from AltaML website homepage with _requests_:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/requests/__init__.py:80: RequestsDependencyWarning: urllib3 (1.25.2) or chardet (3.0.4) doesn't match a supported version!\n",
      "  RequestsDependencyWarning)\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "url = 'https://www.altaml.com/'\n",
    "response = requests.get(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To look at the HTML content, we can try to render it as a a plain-text string, bytes, or json dictionary.  \n",
    "A json dictionary would be very convenient, but this is not guaranteed to work for all webpages (here it will yield an error)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "JSONDecodeError",
     "evalue": "Expecting value: line 1 column 1 (char 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-a2b3c62eb06b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcontent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/lib/python3/dist-packages/requests/models.py\u001b[0m in \u001b[0;36mjson\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    890\u001b[0m                     \u001b[0;31m# used.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m                     \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 892\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcomplexjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    893\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/simplejson/__init__.py\u001b[0m in \u001b[0;36mloads\u001b[0;34m(s, encoding, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, use_decimal, **kw)\u001b[0m\n\u001b[1;32m    516\u001b[0m             \u001b[0mparse_constant\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mobject_pairs_hook\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    517\u001b[0m             and not use_decimal and not kw):\n\u001b[0;32m--> 518\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_default_decoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    519\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m         \u001b[0mcls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mJSONDecoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/simplejson/decoder.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, s, _w, _PY3)\u001b[0m\n\u001b[1;32m    368\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_PY3\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinary_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 370\u001b[0;31m         \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    371\u001b[0m         \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    372\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/simplejson/decoder.py\u001b[0m in \u001b[0;36mraw_decode\u001b[0;34m(self, s, idx, _w, _PY3)\u001b[0m\n\u001b[1;32m    398\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mord0\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0xef\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0midx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'\\xef\\xbb\\xbf'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    399\u001b[0m                 \u001b[0midx\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 400\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscan_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 1 (char 0)"
     ]
    }
   ],
   "source": [
    "content = response.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look directly at the HTML text itself with the _.text_ attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<!DOCTYPE html>\\n<html>\\n<head>\\n    <meta http-equiv=\"Content-Type\" content=\"text/html; charset=UTF-8\">\\n    <meta name=\"viewport\" content=\"initial-scale=1.0, user-scalable=no\" />\\n    <meta name=\"viewport\" content=\"user-scalable=no, width=device-width, height=device-height, initial-scale=1, maximum-scale=1, minimum-scale=1, minimal-ui\" />\\n    <meta name=\"theme-color\" content=\"#20345f\" />\\n    <meta name=\"msapplication-navbutton-color\" content=\"#20345f\" />\\n    <meta name=\"apple-mobile-web-app-status-bar-style\" content=\"#20345f\" />\\n    <meta name=\"description\" content=\"Our focus always starts with the business use-case. We serve as a bridge between cutting-edge academic research and commercialization in industry.\">\\n    <meta name=\"author\" content=\"AltaML Inc.\">\\n    <meta name=\"Geography\" content=\"#1200, 10130 103 street Edmonton, AB T5J 3N9 Canada\">\\n    <meta name=\"Language\" content=\"English\">\\n    <meta name=\"Copyright\" content=\"AltaML Inc.\">\\n    <meta name=\"city\" content=\"Edmonton\">\\n    <me'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content[:1000] # only the first thousand characters as it is a VERY long string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is hard to parse with our eyes, but we could feed this into a function that re-creates the HTML-tree from the string content.  \n",
    "We can \"travel\" through the tree and access elements (this is webscraping)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lxml import html\n",
    "html_tree = html.fromstring(content) # the content is a gigantic, complicated string that we looked at above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Element html at 0x7f46d04e5e08>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "html_tree # we can call methods off of this to find children elements of this HTML document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this HTML tree element, we can use either _xpath_ or _cssselector_ to select elements from the HTML doc.  \n",
    "These are just different ways of asking the HTML-tree to give you specific elements given certain conditions.  \n",
    "As an example, if you visit the homepage and right-click and \"inspect element\" on the main title text \"Machine Learning. Applied.\" we see the HTML element is an _h1_ element with class = \"__title\".  \n",
    "Don't worry about the details of _xpath_ or _cssselector_ at the moment, but to extract this we might do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = html_tree.cssselect('h1.__title')\n",
    "print(title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Even though this is the correct way to \"select\" this _h1_ element with the class __title, it returns nothing!**  \n",
    "Using _requests_, then interpreting the response with the _lxml_ package, and then selecting CSS elements works about 40% of the time.  \n",
    "This is quite easy way to scrape HTML content on _simple_ webpages, however it will spuriously return nothing on more complex pages, even when you're coding it all properly.  \n",
    "**Why is this, and if this isn't the way to web-scrape, what _is_ the way for us to web-scrape effectively?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **The Problem with Python's \"requests\" Package for Web-Scraping HTML Content**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem with this method for retrieving all HTML content from a webpage is that **not all HTML content is immediately loaded upon initiation of the requests.get() command.**  \n",
    "Often times there is a base layer of HTML that renders the bare necessities of the page, followed some short time later by a **javascript call** to load **additional HTML content**.  \n",
    "**This additional content loaded by javascript is not received in the requests.get() call!**  \n",
    "So, even if you right-click-inspect-element the page and are looking at a particular element in the HTML code on the website + browser, there is no guarantee requests.get() actually received that element.  \n",
    "In our example above, the title is loaded by javascript and so we could not \"find\" it in the received HTML document (it's literally not there).  \n",
    "If we could load the webpage, _wait a few seconds_ and _then_ retrieve the HTML document, we would receive _all_ of the loaded HTML content, even those added by javascript."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Selenium: A Package to do both \"bot-like\" Web Navigation and Web Scraping**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selenium is the work-around that allows you to load both the base HTML layer as well as most things loaded some time afterwards by javascript.  \n",
    "Not only this, but Selenium allows you do interact with a website programmatically in the exact same way you might by using a browser.  \n",
    "\n",
    "For example, using Selenium you can:\n",
    "- insert text into search boxes and search on a webpage\n",
    "- click buttons or other interactive media on the webpage\n",
    "- retrieve HTML content just like requests.get()\n",
    "- select HTML elements with _xpath_ and _cssselector_ just like the lxml package\n",
    "- retrieve _more_ HTML content than requests.get() by making selenium \"wait\" for the rest of the content to load through javascript\n",
    "\n",
    "This allows Selenium to be a bit more powerful than just the _requests_ package, in that you can navigate around multiple web-pages in a point-and-click fashion programmatically.  \n",
    "\n",
    "Instructions on installation of Selenium can be found here: https://pypi.org/project/selenium/\n",
    "\n",
    "In the following example, we will scrape the 7-day weather forecast for a few different cities in Alberta using Selenium.  \n",
    "I originally tried this using just _requests_ but the table containing our forecasts is loaded afterwards by javascript and thus we need selenium!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Example: Retrieving this week's forecast from The Weather Network for 5 Alberta cities**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you go to this website:  \n",
    "\n",
    "https://www.theweathernetwork.com/ca/weather/alberta/edmonton  \n",
    "\n",
    "you will see a box somewhere on the page with the 7-day forecast for Edmonton.  \n",
    "\n",
    "Say we had a list of cities in Canada, and wanted to retrieve the 7-day forecast for each city in one go. How would we do this?  \n",
    "\n",
    "The entire process boils down to these steps:\n",
    "\n",
    "- **Step #1**: Identify the website link that pulls up the 7-day forecast\n",
    "- **Step #2**: Ask the website to give you the raw HTML document\n",
    "- **Step #3**: Figure out which elements in this HTML page represent the 7-day forecast\n",
    "- **Step #4**: Extract these values and store them in a DataFrame, a dictionary, etc.  \n",
    "\n",
    "The weather link above was simply an example for Edmonton because we are centered here, which I found by googling \"Edmonton weather\".\n",
    "Notice the URL itself explicitly states the city we are searching!  \n",
    "We can take advantage of this and simply change the URL before pulling the HTML content to represent some specific city.  \n",
    "We _could_ use selenium to actually search the word \"Edmonton\" in a search box on the homepage of the Weather Network, but we don't have to do this here given the simplicity of the URLs created when searching different cities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step #1: Identify the website URLs that pull up the 7-day forecasts\n",
    "Since the format of the weather website is identical for each city, we really only need to write a function that retrieves the forecast for a given city."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_url(city):\n",
    "    \"\"\"Make the url from which to retrieve HTML content for scraping.\"\"\"\n",
    "    base_url = 'https://theweathernetwork.com/ca/weather/alberta/'\n",
    "    url = base_url + city\n",
    "    return url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step #2: Ask the website for the HTML content from which to extract the weather forecasts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is what navigates around the web browser\n",
    "from selenium import webdriver \n",
    "\n",
    "# I use Firefox, but there is an equivalent for Google Chrome - this import isn't actually necessary, \n",
    "# but is used to do everything in the background instead of opening an actual browser every time on your computer.\n",
    "from selenium.webdriver.firefox.options import Options\n",
    "\n",
    "# This is just to parse the actual html tree in the final steps.\n",
    "from lxml import html "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The thing that actually navigates around in your web browser is called the DRIVER.   \n",
    "Instantiating the driver is actually one line of code: _driver = webdriver.Firefox()_.    \n",
    "However, we will add a few conditions to it:\n",
    "\n",
    "- make sure to wait 10 seconds or so for everything to load before scraping the HTML content\n",
    "- suppress the opening of a physical browser on my computer and do it in the background instead\n",
    "\n",
    "I find it visually simple to stick this all in a nice little function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_driver():\n",
    "    \"\"\"Start the selenium driver.\"\"\"\n",
    "    # Suppress actual physical browser opening, put in background.\n",
    "    options = Options()\n",
    "    options.add_argument('--headless') \n",
    "    # Initiate browser session with the above options\n",
    "    driver = webdriver.Firefox(options=options)\n",
    "    wait_time = 10 # seconds\n",
    "    driver.implicitly_wait(wait_time) # this lets webdriver wait 10 seconds for the website to load\n",
    "    return(driver)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's start the driver, and ask it to retrieve the HTML content from the Edmonton Weather Network page shown a few cells above.  \n",
    "Just like _requests.get()_, we will use the equivalent _driver.get()_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the search URL.\n",
    "url = make_url('edmonton')\n",
    "# Start the driver.\n",
    "driver = start_driver()\n",
    "# Get the driver to get the page.\n",
    "driver.get(url)\n",
    "# Get the HTML from the page.\n",
    "html_source = driver.page_source\n",
    "# Push this big string of HTML through lxml.html() to make it a tree object that is easily searchable/navigated for specific elements.\n",
    "html = html.fromstring(html_source)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check out this html object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Element html at 0x7fbcbc1cb9f8>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perfect! Now we've retrieved the HTML content, we can pull text from specific elements.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step #3: Figure out which elements in the HTML tree represent the 7-day forecast.  \n",
    "First, let's identify the specific data items we would like from the forecast, and then we will poke around in the HTML from the page to find it.  \n",
    "\n",
    "<img src='../img/webscraping-weather-1.png' > "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each day, let's extract:\n",
    "\n",
    "- the day of the week\n",
    "- the text-forecast (eg. Rain, Risk of a thunderstorm, mainly sunny, etc.)\n",
    "- the temperature\n",
    "- the POP (percent-of-precipitation)\n",
    "\n",
    "We could extract all of this information just as easily, but for brevity of this example we will keep it to three elements.  \n",
    "\n",
    "Now that we've decided the data to scrape, where exactly in the HTML tree are these values? To know this, we have to physically visit the webpage (at least this one time) and snoop around in the HTML.  \n",
    "\n",
    "Head over to https://www.theweathernetwork.com/ca/weather/alberta/edmonton, right-click on the page (specifically, on something you'd like to identify in the HTML tree) and click \"Inspect\" or \"Inspect Element\".  \n",
    "\n",
    "I recommend right-clicking and inspecting where the first column says \"Fri\" (or whatever day of the week it is when you are reading this!)  \n",
    "This is what I see (in Firefox - the inspect tools should look similar in Chrome):\n",
    "\n",
    "<img src='../img/webscraping-weather-2.png' >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By scrolling around the HTML code, the page will highlight page objects and tell you what the piece of HTML actually represents.  \n",
    "It appears that each column of the table (each day's information) is stuck inside a _div_ element called _<div class=\"wxColumn wxColumn-seven dotw_5\">.  \n",
    "    \n",
    "Actually, by closing this element, I notice that actually each column is represented by a similar div element, and the day is represented as an integer from 0 - 6 at the end of the \"class\" of the div.\n",
    "    \n",
    "<img src='../img/webscraping-weather-3.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have located in the HTML the elements that contain our weather information.   \n",
    "The entire table is div with a class \"divTableBody\" while each column is a child div of this with a unique class name.  \n",
    "We could just extract these div elements from our HTML tree, but the div elements themselves are sort of meaningless - we want some information INSIDE each of these div elements.  \n",
    "So, we should expand them and look around for the day, the temperature and the POP.\n",
    "\n",
    "<img src='../img/webscraping-weather-4.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The day of the week is inside a SPAN element with the class=\"day\".  \n",
    "\n",
    "The \"directions\" in the HTML tree to this element should look something like this:\n",
    "\n",
    "- For Friday, go to the DIV with the class=\"wxColumn wxColumn-seven dotw_5\" because 5 is the number representing Friday (where Monday is 1)\n",
    "- Inside of this div, go into the DIV with class=\"wxRow\"\n",
    "- Inside of this div, go into the SPAN with class=\"day\"\n",
    "- Grab the text inside this last SPAN element\n",
    "\n",
    "This is admittedly a bit tedious, and there are quicker ways to extract these elements.\n",
    "\n",
    "We can construct this \"path\" in the HTML tree by using **xpath**, a method of the html document (like XML-path).  \n",
    "There is a \"language\" that describes a path to an element in the tree which **xpath** understands.  \n",
    "Equivalently, we could use **css selectors**, a selection \"language\" very similar **xpath**.   \n",
    "Use whichever you prefer - here I will use **xpath**.\n",
    "You don't need to know everything, but we'll go through the basics experientially.\n",
    "\n",
    "**NOTE ON SELENIUM AND XPATHS**: Selenium _does_ have the capability of finding elements by xpath and css selectors, which begs the question - why import the lxml package and construct the HTML tree from this at all? Essentially, we could use the following:\n",
    "\n",
    "**driver.find_element_by_xpath(some-xpath-string)**\n",
    "\n",
    "The problem is that Selenium can find elements, but not _text_ inside elements (I have no idea why they wouldn't implement this capability).\n",
    "\n",
    "So, a work-around is to import lxml and do the xpath selection off of this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<Element span at 0x7fbcbc7fe3b8>,\n",
       " <Element span at 0x7fbcbc7eff48>,\n",
       " <Element span at 0x7fbcbc1cbae8>,\n",
       " <Element span at 0x7fbcbc1cbb88>,\n",
       " <Element span at 0x7fbcbc1cbbd8>,\n",
       " <Element span at 0x7fbcbc1cbb38>,\n",
       " <Element span at 0x7fbcbc1cbc28>]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The double slash jumps to ANY span element in the document - a single / means you are working down the tree in an absolute path from the very top.\n",
    "# The condition in the square brackets is that the span element must have the class tag called \"day\".\n",
    "# The same would work for some id: example, if id='altaml' we could write \"//span[@id='altaml']\".\n",
    "# Remember to use double quote on the outside, and singles on the inside, or vice-versa.\n",
    "html.xpath(\"//span[@class='day']\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perfect! There are 7 _span_ elements with class=\"day\", representing all 7 days of the week. To retrieve the text from these elements, just add /text() after inside the xpath:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun', 'Mon']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "html.xpath(\"//span[@class='day']/text()\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These appear unordered from the image (the image starts at Friday), as I started this notebook on a Thursday, and returned to edit on Friday, meaning the next day in the forecast is really Saturday.  \n",
    "Generally, these should be in the order they appear in the HTML document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define some functions that will pull out the info we want:\n",
    "\n",
    "- the day of the week\n",
    "- the text-forecast (eg. Rain, Risk of a thunderstorm, mainly sunny, etc.)\n",
    "- the temperature\n",
    "- the POP (percent-of-precipitation)\n",
    "\n",
    "The input to each of these functions is the \"day element\" that defines each column, since inside each of these is the identical info of the weather forecast.\n",
    "These elements here:\n",
    "\n",
    "<img src='../img/webscraping-weather-3.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the day xpaths of the day elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "sunday = \"//div[@class='wxColumn wxColumn-seven dotw_0']\"\n",
    "monday = \"//div[@class='wxColumn wxColumn-seven dotw_1']\"\n",
    "tuesday = \"//div[@class='wxColumn wxColumn-seven dotw_2']\"\n",
    "wednesday = \"//div[@class='wxColumn wxColumn-seven dotw_3']\"\n",
    "thursday = \"//div[@class='wxColumn wxColumn-seven dotw_4']\"\n",
    "friday = \"//div[@class='wxColumn wxColumn-seven dotw_5']\"\n",
    "saturday = \"//div[@class='wxColumn wxColumn-seven dotw_6']\"\n",
    "\n",
    "days = [sunday, monday, tuesday, wednesday, thursday, friday, saturday]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each of these elements has it's own xpath, which can access all of their children elements.   \n",
    "Make functions to extract info from each of these day elements, and search around in the Inspect-Element mode to find where each of the attributes we want are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dow(day_path):\n",
    "    \"\"\"Get the day of the week from the HTML content.\"\"\"\n",
    "    dow_path = \"//span[@class='day']/text()\"\n",
    "    xpath = day_path + dow_path\n",
    "    dow = html.xpath(xpath)[0]\n",
    "    return(dow)\n",
    "    \n",
    "def get_forecast(day_path):\n",
    "    \"\"\"Get the forecast (text) from the HTML content.\"\"\"\n",
    "    forecast_path = \"//span[@class='wx_description daytime']/text()\"\n",
    "    xpath = day_path + forecast_path\n",
    "    forecast = html.xpath(xpath)[0]\n",
    "    return(forecast)\n",
    "    \n",
    "def get_temperature(day_path):\n",
    "    \"\"\"Get the temperature from the HTML content.\"\"\"\n",
    "    temperature_path = \"//span[@class='wxperiod_temp daytime']/text()\"\n",
    "    xpath = day_path + temperature_path\n",
    "    temperature = html.xpath(xpath)[0]\n",
    "    return(temperature)\n",
    "    \n",
    "def get_pop(day_path):\n",
    "    \"\"\"Get the POP (percent of precipitation) from the HTML content.\"\"\"\n",
    "    pop_path = \"//span[@class='wxObs daytime']/text()\"\n",
    "    xpath = day_path + pop_path\n",
    "    pop = html.xpath(xpath)[0]\n",
    "    return(pop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weather forecast for Sun: Mainly sunny, 16 degrees C, POP% = 15\n",
      "Weather forecast for Mon: Chance of a shower, 18 degrees C, POP% = 18\n",
      "Weather forecast for Tue: Mainly sunny, 20 degrees C, POP% = 20\n",
      "Weather forecast for Wed: Mainly sunny, 23 degrees C, POP% = 24\n",
      "Weather forecast for Thu: A mix of sun and clouds, 22 degrees C, POP% = 23\n",
      "Weather forecast for Fri: A mix of sun and clouds, 21 degrees C, POP% = 22\n",
      "Weather forecast for Sat: A mix of sun and clouds, 13 degrees C, POP% = 11\n"
     ]
    }
   ],
   "source": [
    "for day in days:\n",
    "    dow = get_dow(day)\n",
    "    forecast = get_forecast(day)\n",
    "    temperature = get_temperature(day)\n",
    "    pop = get_pop(day)\n",
    "    \n",
    "    print(f\"Weather forecast for {dow}: {forecast}, {temperature} degrees C, POP% = {pop}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Ta-da! We've scraped weather data from The Weather Network for Edmonton.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**To access only the source-code for this project, please see the relevant python file in the same repository (alberta-weather-webscraping.py).**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
