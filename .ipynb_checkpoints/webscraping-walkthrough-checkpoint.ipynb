{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Example: Retrieving this week's forecast from The Weather Network for 5 Alberta cities**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you go to this website:  \n",
    "\n",
    "https://www.theweathernetwork.com/ca/weather/alberta/edmonton  \n",
    "\n",
    "you will see a box somewhere on the page with the 7-day forecast for Edmonton.  \n",
    "\n",
    "Say we had a list of cities in Canada, and wanted to retrieve the 7-day forecast for each city in one go. How would we do this?  \n",
    "\n",
    "The entire process boils down to these steps:\n",
    "\n",
    "- **Step #1**: Identify the website link that pulls up the 7-day forecast\n",
    "- **Step #2**: Ask the website to give you the raw HTML document\n",
    "- **Step #3**: Figure out which elements in this HTML page represent the 7-day forecast\n",
    "- **Step #4**: Extract these values and store them in a DataFrame, a dictionary, etc.  \n",
    "\n",
    "The weather link above was simply an example for Edmonton because we are centered here, which I found by googling \"Edmonton weather\".\n",
    "Notice the URL itself explicitly states the city we are searching!  \n",
    "We can take advantage of this and simply change the URL before pulling the HTML content to represent some specific city.  \n",
    "We _could_ use selenium to actually search the word \"Edmonton\" in a search box on the homepage of the Weather Network, but we don't have to do this here given the simplicity of the URLs created when searching different cities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step #1: Identify the website URLs that pull up the 7-day forecasts\n",
    "Since the format of the weather website is identical for each city, we really only need to write a function that retrieves the forecast for a given city."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_url(city):\n",
    "    \"\"\"Make the url from which to retrieve HTML content for scraping.\"\"\"\n",
    "    base_url = 'https://theweathernetwork.com/ca/weather/alberta/'\n",
    "    url = base_url + city\n",
    "    return url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step #2: Ask the website for the HTML content from which to extract the weather forecasts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is what navigates around the web browser\n",
    "from selenium import webdriver \n",
    "\n",
    "# I use Firefox, but there is an equivalent for Google Chrome - this import isn't actually necessary, \n",
    "# but is used to do everything in the background instead of opening an actual browser every time on your computer.\n",
    "from selenium.webdriver.firefox.options import Options\n",
    "\n",
    "# This is just to parse the actual html tree in the final steps.\n",
    "from lxml import html "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The thing that actually navigates around in your web browser is called the DRIVER.   \n",
    "Instantiating the driver is actually one line of code: _driver = webdriver.Firefox()_.    \n",
    "However, we will add a few conditions to it:\n",
    "\n",
    "- make sure to wait 10 seconds or so for everything to load before scraping the HTML content\n",
    "- suppress the opening of a physical browser on my computer and do it in the background instead\n",
    "\n",
    "I find it visually simple to stick this all in a nice little function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_driver():\n",
    "    \"\"\"Start the selenium driver.\"\"\"\n",
    "    # Suppress actual physical browser opening, put in background.\n",
    "    options = Options()\n",
    "    options.add_argument('--headless') \n",
    "    # Initiate browser session with the above options\n",
    "    driver = webdriver.Firefox(options=options)\n",
    "    wait_time = 10 # seconds\n",
    "    driver.implicitly_wait(wait_time) # this lets webdriver wait 10 seconds for the website to load\n",
    "    return(driver)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's start the driver, and ask it to retrieve the HTML content from the Edmonton Weather Network page shown a few cells above.  \n",
    "Just like _requests.get()_, we will use the equivalent _driver.get()_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the search URL.\n",
    "url = make_url('edmonton')\n",
    "# Start the driver.\n",
    "driver = start_driver()\n",
    "# Get the driver to get the page.\n",
    "driver.get(url)\n",
    "# Get the HTML from the page.\n",
    "html_source = driver.page_source\n",
    "# Push this big string of HTML through lxml.html() to make it a tree object that is easily searchable/navigated for specific elements.\n",
    "html = html.fromstring(html_source)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check out this html object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Element html at 0x7fcf6c06b368>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perfect! Now we have an HTML-tree, just like a real HTML document, from which we can pull text from specific elements.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step #3: Figure out which elements in the HTML tree represent the 7-day forecast.  \n",
    "First, let's identify the specific data items we would like from the forecast, and then we will poke around in the HTML from the page to find it.  \n",
    "\n",
    "<img src='img/webscraping-weather-1.png' > "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each day, let's extract:\n",
    "\n",
    "- the day of the week\n",
    "- the text-forecast (eg. Rain, Risk of a thunderstorm, mainly sunny, etc.)\n",
    "- the temperature\n",
    "- the POP (percent-of-precipitation)\n",
    "\n",
    "We could extract all of this information just as easily, but for brevity of this example we will keep it to three elements.  \n",
    "\n",
    "Now that we've decided the data to scrape, where exactly in the HTML tree are these values? To know this, we have to physically visit the webpage (at least this one time) and snoop around in the HTML.  \n",
    "\n",
    "Head over to https://www.theweathernetwork.com/ca/weather/alberta/edmonton, right-click on the page (specifically, on something you'd like to identify in the HTML tree) and click \"Inspect\" or \"Inspect Element\".  \n",
    "\n",
    "I recommend right-clicking and inspecting where the first column says \"Fri\" (or whatever day of the week it is when you are reading this!)  \n",
    "This is what I see (in Firefox - the inspect tools should look similar in Chrome):\n",
    "\n",
    "<img src='img/webscraping-weather-2.png' >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By scrolling around the HTML code, the page will highlight page objects and tell you what the piece of HTML actually represents.  \n",
    "It appears that each column of the table (each day's information) is stuck inside a _div_ element called _<div class=\"wxColumn wxColumn-seven dotw_5\">.  \n",
    "    \n",
    "Actually, by closing this element, I notice that actually each column is represented by a similar div element, and the day is represented as an integer from 0 - 6 at the end of the \"class\" of the div.\n",
    "    \n",
    "<img src='img/webscraping-weather-3.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have located in the HTML the elements that contain our weather information.   \n",
    "The entire table is div with a class \"divTableBody\" while each column is a child div of this with a unique class name.  \n",
    "We could just extract these div elements from our HTML tree, but the div elements themselves are sort of meaningless - we want some information INSIDE each of these div elements.  \n",
    "So, we should expand them and look around for the day, the temperature and the POP.\n",
    "\n",
    "<img src='img/webscraping-weather-4.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The day of the week is inside a SPAN element with the class=\"day\".  \n",
    "\n",
    "The \"directions\" in the HTML tree to this element should look something like this:\n",
    "\n",
    "- For Friday, go to the DIV with the class=\"wxColumn wxColumn-seven dotw_5\" because 5 is the number representing Friday (where Monday is 1)\n",
    "- Inside of this div, go into the DIV with class=\"wxRow\"\n",
    "- Inside of this div, go into the SPAN with class=\"day\"\n",
    "- Grab the text inside this last SPAN element\n",
    "\n",
    "This is admittedly a bit tedious, and there are quicker ways to extract these elements.\n",
    "\n",
    "We can construct this \"path\" in the HTML tree by using **xpath**, a method of the html document (like XML-path).  \n",
    "There is a \"language\" that describes a path to an element in the tree which **xpath** understands.  \n",
    "Equivalently, we could use **css selectors**, a method of the html document just like **xpath**.   \n",
    "This is also a \"language\" that describes a path to an element in the tree.  \n",
    "Use whichever you prefer - here I will use **xpath**.\n",
    "You don't need to know everything, but we'll go through the basics experientially."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<Element span at 0x7fcf6c06b818>,\n",
       " <Element span at 0x7fcf6c06b778>,\n",
       " <Element span at 0x7fcf6c06b4a8>,\n",
       " <Element span at 0x7fcf6c06b4f8>,\n",
       " <Element span at 0x7fcf6c06b7c8>,\n",
       " <Element span at 0x7fcf6c06b868>,\n",
       " <Element span at 0x7fcf6c06b908>]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The double slash jumps to ANY span element in the document - a single / means you are working down the tree in an absolute path from the very top.\n",
    "# The condition in the square brackets is that the span element must have the class tag called \"day\".\n",
    "# Remember to use double quote on the outside, and singles on the inside, or vice-versa.\n",
    "html.xpath(\"//span[@class='day']\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perfect! There are 7 _span_ elements with class=\"day\", representing all 7 days of the week. To retrieve the text from these elements, just add /text() after inside the xpath:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Sat', 'Sun', 'Mon', 'Tue', 'Wed', 'Thu', 'Fri']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "html.xpath(\"//span[@class='day']/text()\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These appear unordered from the image (the image starts at Friday), as I started this notebook on a Thursday, and returned to edit on Friday, meaning the next day in the forecast is really Saturday.  \n",
    "Generally, these should be in the order they appear in the HTML document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define some functions that will pull out the info we want:\n",
    "\n",
    "- the day of the week\n",
    "- the text-forecast (eg. Rain, Risk of a thunderstorm, mainly sunny, etc.)\n",
    "- the temperature\n",
    "- the POP (percent-of-precipitation)\n",
    "\n",
    "The input to each of these functions is the \"day element\" that defines each column, since inside each of these is the identical info of the weather forecast.\n",
    "These elements here:\n",
    "\n",
    "<img src='img/webscraping-weather-3.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the day xpaths of the day elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "sunday = \"//div[@class='wxColumn wxColumn-seven dotw_0']\"\n",
    "monday = \"//div[@class='wxColumn wxColumn-seven dotw_1']\"\n",
    "tuesday = \"//div[@class='wxColumn wxColumn-seven dotw_2']\"\n",
    "wednesday = \"//div[@class='wxColumn wxColumn-seven dotw_3']\"\n",
    "thursday = \"//div[@class='wxColumn wxColumn-seven dotw_4']\"\n",
    "friday = \"//div[@class='wxColumn wxColumn-seven dotw_5']\"\n",
    "saturday = \"//div[@class='wxColumn wxColumn-seven dotw_6']\"\n",
    "\n",
    "days = [sunday, monday, tuesday, wednesday, thursday, friday, saturday]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each of these elements has it's own xpath, which can access all of their children elements.   \n",
    "Make functions to extract info from each of these day elements, and search around in the Inspect-Element mode to find where each of the attributes we want are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dow(day_path):\n",
    "    \"\"\"Get the day of the week from the HTML content.\"\"\"\n",
    "    dow_path = \"//span[@class='day']/text()\"\n",
    "    xpath = day_path + dow_path\n",
    "    dow = html.xpath(xpath)[0]\n",
    "    return(dow)\n",
    "    \n",
    "def get_forecast(day_path):\n",
    "    \"\"\"Get the forecast (text) from the HTML content.\"\"\"\n",
    "    forecast_path = \"//span[@class='wx_description daytime']/text()\"\n",
    "    xpath = day_path + forecast_path\n",
    "    forecast = html.xpath(xpath)[0]\n",
    "    return(forecast)\n",
    "    \n",
    "def get_temperature(day_path):\n",
    "    \"\"\"Get the temperature from the HTML content.\"\"\"\n",
    "    temperature_path = \"//span[@class='wxperiod_temp daytime']/text()\"\n",
    "    xpath = day_path + temperature_path\n",
    "    temperature = html.xpath(xpath)[0]\n",
    "    return(temperature)\n",
    "    \n",
    "def get_pop(day_path):\n",
    "    \"\"\"Get the POP (percent of precipitation) from the HTML content.\"\"\"\n",
    "    pop_path = \"//span[@class='wxObs daytime']/text()\"\n",
    "    xpath = day_path + pop_path\n",
    "    pop = html.xpath(xpath)[0]\n",
    "    return(pop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weather forecast for Sun: Mainly sunny, 16 degrees C, POP% = 15\n",
      "Weather forecast for Mon: Chance of a shower, 18 degrees C, POP% = 18\n",
      "Weather forecast for Tue: Mainly sunny, 20 degrees C, POP% = 20\n",
      "Weather forecast for Wed: Mainly sunny, 23 degrees C, POP% = 24\n",
      "Weather forecast for Thu: A mix of sun and clouds, 22 degrees C, POP% = 23\n",
      "Weather forecast for Fri: A mix of sun and clouds, 21 degrees C, POP% = 22\n",
      "Weather forecast for Sat: A mix of sun and clouds, 13 degrees C, POP% = 11\n"
     ]
    }
   ],
   "source": [
    "for day in days:\n",
    "    dow = get_dow(day)\n",
    "    forecast = get_forecast(day)\n",
    "    temperature = get_temperature(day)\n",
    "    pop = get_pop(day)\n",
    "    \n",
    "    print(f\"Weather forecast for {dow}: {forecast}, {temperature} degrees C, POP% = {pop}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ta-da! We've scraped weather data from The Weather Network for Edmonton."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your patience is admired. If you'd like to see only the relevant bits of code and run this easily from a single file, see the file in the local folder _\"alberta-weather-webscraping-code.py\"_.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
